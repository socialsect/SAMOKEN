from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import torch
import io
from PIL import Image
import numpy as np
import time
import asyncio
from functools import lru_cache

# ‚úÖ --- FIX for Windows 'PosixPath' error ---
import pathlib
import platform
if platform.system() == 'Windows':
    temp = pathlib.PosixPath
    pathlib.PosixPath = pathlib.WindowsPath

app = FastAPI()

# Enable CORS with optimized settings
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["POST", "GET"],
    allow_headers=["*"],
)
 
# Global variables for optimizations
model = None
device = None
model_loaded = False
inference_cache = {}
cache_size = 0
MAX_CACHE_SIZE = 50

def load_yolov10_model():
    """Properly load YOLOv10 model with correct interface"""
    global model, device, model_loaded
    
    try:
        # Determine best device
        if torch.cuda.is_available():
            device = torch.device('cuda')
            print("üöÄ Using GPU acceleration")
        else:
            device = torch.device('cpu')
            print("üíª Using CPU processing")
        
        # Method 1: Try ultralytics YOLO (recommended for YOLOv10)
        try:
            from ultralytics import YOLO
            model = YOLO('best10.pt')
            
            # Configure model for optimal performance
            model.overrides['conf'] = 0.15  # Lower confidence threshold
            model.overrides['iou'] = 0.4    # IoU threshold
            model.overrides['max_det'] = 5  # Limit detections
            model.overrides['half'] = torch.cuda.is_available()  # FP16 on GPU
            
            print("‚úÖ Successfully loaded YOLOv10 model using ultralytics.YOLO")
            print(f"üìã Model info: {model.info()}")
            model_loaded = True
            return
            
        except ImportError:
            print("‚ö†Ô∏è ultralytics package not found, trying alternative method...")
        except Exception as e:
            print(f"‚ö†Ô∏è ultralytics loading failed: {e}, trying alternative...")
        
        # Method 2: Try torch.hub with YOLOv5 interface (fallback)
        try:
            # Load using YOLOv5 interface but specify it's a custom model
            model = torch.hub.load('ultralytics/yolov5', 'custom', path='best10.pt', force_reload=True)
            model = model.to(device)
            
            model.eval()
            
            # Apply optimizations
            if hasattr(model, 'half') and device.type == 'cuda':
                model.half()
                print("üî• Enabled FP16 precision for GPU")
            
            # Set inference parameters
            model.conf = 0.15
            model.iou = 0.4
            model.max_det = 5
            
            print("‚úÖ Successfully loaded model using torch.hub (YOLOv5 interface)")
            model_loaded = True
            return
            
        except Exception as e:
            print(f"‚ùå torch.hub loading failed: {e}")
        
        # Method 3: Direct torch loading (last resort)
        try:
            model = torch.load('best10.pt', map_location=device)
            if hasattr(model, 'eval'):
                model.eval()
            print("‚ö†Ô∏è Loaded model using direct torch.load (may have compatibility issues)")
            model_loaded = True
            
        except Exception as e:
            print(f"‚ùå All loading methods failed: {e}")
            model_loaded = False
    
    except Exception as e:
        print(f"‚ùå Critical error in model loading: {e}")
        model_loaded = False

# Initialize model on startup
@app.on_event("startup")
async def startup_event():
    load_yolov10_model()

# Revert Windows path fix
if platform.system() == 'Windows':
    pathlib.PosixPath = temp

# Optimized image preprocessing
@lru_cache(maxsize=10)
def get_optimal_size(original_width: int, original_height: int, max_size: int = 640):
    """Calculate optimal resize dimensions while maintaining aspect ratio"""
    if max(original_width, original_height) <= max_size:
        return original_width, original_height
    
    if original_width > original_height:
        new_width = max_size
        new_height = int((max_size * original_height) / original_width)
    else:
        new_height = max_size
        new_width = int((max_size * original_width) / original_height)
    
    # Ensure dimensions are multiples of 32 for YOLO
    new_width = (new_width // 32) * 32
    new_height = (new_height // 32) * 32
    
    return max(32, new_width), max(32, new_height)

def preprocess_image_fast(image: Image.Image) -> Image.Image:
    """Fast image preprocessing with optimizations"""
    original_width, original_height = image.size
    new_width, new_height = get_optimal_size(original_width, original_height, 640)
    
    if (new_width, new_height) != (original_width, original_height):
        image = image.resize((new_width, new_height), Image.BILINEAR)
    
    return image

def post_process_detections_v10(results, original_size: tuple, processed_size: tuple, confidence_threshold: float = 0.15):
    """Process YOLOv10 results with proper format handling"""
    detections = []
    
    # Calculate scaling factors
    scale_x = original_size[0] / processed_size[0]
    scale_y = original_size[1] / processed_size[1]
    
    try:
        # Handle ultralytics YOLO format (YOLOv8/v10)
        if hasattr(results, '__len__') and len(results) > 0:
            result = results[0]  # First result
            
            if hasattr(result, 'boxes') and result.boxes is not None:
                boxes = result.boxes
                
                # Extract data
                if hasattr(boxes, 'xyxy') and boxes.xyxy is not None:
                    coords = boxes.xyxy.cpu().numpy()
                    confs = boxes.conf.cpu().numpy() if boxes.conf is not None else [1.0] * len(coords)
                    classes = boxes.cls.cpu().numpy() if boxes.cls is not None else [0] * len(coords)
                    
                    for i, (box_coords, conf, cls) in enumerate(zip(coords, confs, classes)):
                        if conf >= confidence_threshold:
                            x1, y1, x2, y2 = box_coords
                            
                            # Scale back to original coordinates
                            x1_orig = x1 * scale_x
                            y1_orig = y1 * scale_y
                            x2_orig = x2 * scale_x
                            y2_orig = y2 * scale_y
                            
                            x_center = (x1_orig + x2_orig) / 2
                            y_center = (y1_orig + y2_orig) / 2
                            
                            detections.append({
                                "x": float(x_center),
                                "y": float(y_center),
                                "confidence": float(conf),
                                "class_id": int(cls),
                                "class_name": "golf_ball",
                                "box": [float(x1_orig), float(y1_orig), float(x2_orig), float(y2_orig)]
                            })
        
        # Handle torch.hub YOLOv5 format (fallback)
        elif hasattr(results, 'xyxy') and len(results.xyxy) > 0:
            boxes = results.xyxy[0].cpu().numpy()
            
            for box in boxes:
                if len(box) >= 6:
                    x1, y1, x2, y2, conf, cls = box[:6]
                    if conf >= confidence_threshold:
                        # Scale back to original coordinates
                        x1_orig = x1 * scale_x
                        y1_orig = y1 * scale_y
                        x2_orig = x2 * scale_x
                        y2_orig = y2 * scale_y
                        
                        x_center = (x1_orig + x2_orig) / 2
                        y_center = (y1_orig + y2_orig) / 2
                        
                        detections.append({
                            "x": float(x_center),
                            "y": float(y_center),
                            "confidence": float(conf),
                            "class_id": int(cls),
                            "class_name": "golf_ball",
                            "box": [float(x1_orig), float(y1_orig), float(x2_orig), float(y2_orig)]
                        })
        
        else:
            print("‚ö†Ô∏è Unknown result format - please check YOLO version compatibility")
    
    except Exception as e:
        print(f"‚ö†Ô∏è Error in post-processing: {e}")
    
    return detections

# Simple request rate limiting
request_times = []
MAX_REQUESTS_PER_SECOND = 20

def is_rate_limited():
    global request_times
    now = time.time()
    request_times = [t for t in request_times if now - t < 1.0]
    
    if len(request_times) >= MAX_REQUESTS_PER_SECOND:
        return True
    
    request_times.append(now)
    return False

@app.post("/analyze-ball/")
async def analyze_ball_v10(file: UploadFile = File(...)):
    """
    YOLOv10 optimized ball detection endpoint
    """
    if not model_loaded:
        raise HTTPException(status_code=503, detail="YOLOv10 model not loaded")
    
    if is_rate_limited():
        raise HTTPException(status_code=429, detail="Rate limit exceeded")
    
    start_time = time.time()
    
    try:
        contents = await file.read()
        
        # Simple caching
        image_hash = hash(contents) % 10000
        global inference_cache, cache_size
        
        if image_hash in inference_cache:
            cached_result = inference_cache[image_hash]
            print(f"üìã Cache hit! Returning cached result")
            return cached_result
        
        # Load and preprocess image
        image = Image.open(io.BytesIO(contents)).convert("RGB")
        original_size = image.size
        processed_image = preprocess_image_fast(image)
        processed_size = processed_image.size
        
        # Run inference based on model type
        with torch.no_grad():
            if hasattr(model, 'predict'):
                # ultralytics YOLO interface
                results = model.predict(
                    processed_image,
                    conf=0.15,
                    iou=0.4,
                    max_det=5,
                    half=torch.cuda.is_available(),
                    verbose=False
                )
            else:
                # torch.hub interface
                results = model(processed_image)
        
        # Post-process results
        detections = post_process_detections_v10(results, original_size, processed_size, confidence_threshold=0.15)
        
        # Filter and sort
        high_confidence_detections = [
            det for det in detections 
            if det['confidence'] >= 0.2
        ]
        high_confidence_detections.sort(key=lambda x: x['confidence'], reverse=True)
        final_detections = high_confidence_detections[:3]
        
        processing_time = (time.time() - start_time) * 1000
        
        result = {
            "detections": final_detections,
            "processing_time_ms": round(processing_time, 1),
            "image_size": original_size,
            "processed_size": processed_size,
            "model_type": "YOLOv10"
        }
        
        # Cache management
        if cache_size < MAX_CACHE_SIZE:
            inference_cache[image_hash] = result
            cache_size += 1
        elif len(inference_cache) >= MAX_CACHE_SIZE:
            oldest_key = next(iter(inference_cache))
            del inference_cache[oldest_key]
            inference_cache[image_hash] = result
        
        print(f"üéØ YOLOv10 processed in {processing_time:.1f}ms, found {len(final_detections)} detections")
        
        return result
    
    except Exception as e:
        error_time = (time.time() - start_time) * 1000
        print(f"‚ùå YOLOv10 error in {error_time:.1f}ms: {str(e)}")
        return {
            "detections": [], 
            "error": str(e),
            "processing_time_ms": round(error_time, 1),
            "model_type": "YOLOv10"
        }

@app.get("/health")
async def health_check():
    model_info = "Unknown"
    if model_loaded:
        if hasattr(model, 'predict'):
            model_info = "YOLOv10 (ultralytics)"
        elif hasattr(model, 'xyxy'):
            model_info = "YOLOv5-interface"
        else:
            model_info = "Custom torch model"
    
    return {
        "status": "healthy" if model_loaded else "model_not_loaded",
        "model_loaded": model_loaded,
        "model_type": model_info,
        "model_file": "best10.pt",
        "device": str(device) if device else "unknown",
        "gpu_available": torch.cuda.is_available(),
        "cache_size": len(inference_cache)
    }

@app.get("/model-info")
async def model_info():
    if not model_loaded:
        return {"error": "Model not loaded"}
    
    info = {
        "model_file": "best10.pt",
        "device": str(device) if device else "unknown",
        "model_interface": "ultralytics" if hasattr(model, 'predict') else "torch.hub",
        "optimizations": [
            "FP16 precision" if torch.cuda.is_available() else "FP32 precision",
            "Image preprocessing optimization",
            "Result caching",
            "Rate limiting"
        ]
    }
    
    # Try to get model-specific info
    try:
        if hasattr(model, 'names'):
            info["classes"] = model.names
        if hasattr(model, 'info'):
            info["model_summary"] = str(model.info())
    except:
        pass
    
    return info

if __name__ == "__main__":
    import uvicorn
    print("üöÄ Starting YOLOv10 Golf Ball Detection Server...")
    print("üìÅ Model file: best10.pt")
    print("üîß Will attempt multiple loading methods for best compatibility")
    print("üåê Server starting...")
    
    uvicorn.run(
        app, 
        host="0.0.0.0", 
        port=8000,
        workers=1,
        loop="asyncio",
        log_level="info"
    )